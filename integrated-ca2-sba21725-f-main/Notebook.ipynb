{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "484a3df3",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36abc48c-2191-49ed-8bbc-4ef9e9ca4bf7",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Requirements and imports\n",
    "2. Database Connection and Setup\n",
    "3. Retrieve Data from MySQL\n",
    "4. Sentiment Analysis (for all companies)\n",
    "5. Retrieve Stockprice and left join with Sentiment (for AAPL)\n",
    "6. Descriptive Statistics (for AAPL)\n",
    "7. ARIMA Model (for all companies)\n",
    "8. Neural Network Model (for AAPL)\n",
    "9. LSTM (for all Companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eaaad2-a94b-44f0-98e6-7b2b5f2cc8a9",
   "metadata": {},
   "source": [
    "# Requirements and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc52e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import datetime\n",
    "\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "import keras_tuner as kt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "import pymysql \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from skimpy import skim\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sqlalchemy import create_engine \n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from summarytools import dfSummary\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c6ac5c-cbc9-49a9-816d-135ef7a2ebd8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e727f-eb5c-49f2-9def-3505664a9d3c",
   "metadata": {},
   "source": [
    "# Database Connection and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b31373-d017-45b3-97a9-6add04388f79",
   "metadata": {},
   "source": [
    "### 1. MySQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a81c6-99db-476f-a7e2-d56abcb9451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data from the environment variables\n",
    "hostname = os.getenv('MYSQLHOST')\n",
    "port=int(os.getenv('MYSQLPORT'))\n",
    "username = os.getenv('MYSQLUSR')\n",
    "password = os.getenv('MYSQLPASS')\n",
    "database_name = os.getenv('MYSQLDB') \n",
    "ca_cert_path = '../ca.pem'\n",
    "\n",
    "# Create MySQL connection object\n",
    "connection = pymysql.connect( \n",
    "    host=hostname, \n",
    "    port=port, \n",
    "    user=username, \n",
    "    password=password, \n",
    "    database=database_name, \n",
    "    ssl={'ca': ca_cert_path} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ccb02-7a35-4c46-b315-ecea341ce3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SQLAlchemy engine for MySQL\n",
    "engine = create_engine(f\"mysql+pymysql://{username}:{password}@{hostname}:{port}/{database_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a8621-9f1b-4ac2-8faf-06500892f9e8",
   "metadata": {},
   "source": [
    "### 2. MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb6fed-6262-4165-a2d8-bdd3cadc477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGOURI = os.getenv('MONGOURI')\n",
    "MONGODB = os.getenv('MONGODB')\n",
    "\n",
    "client = MongoClient(MONGOURI, server_api=ServerApi('1'))\n",
    "\n",
    "# Select database by name.\n",
    "# If database does not exist it will create a new one.\n",
    "db = client[MONGODB]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae1776-38e3-429a-b4b6-99f62d54edf5",
   "metadata": {},
   "source": [
    "## MongoDB collections setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7862d-a2c9-4d47-b44d-602843c26051",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Collections used by this notebook (Select or create)\n",
    "arima_coll = db[\"arima\"]\n",
    "lstm_coll = db[\"lstm\"]\n",
    "sentiment_coll = db[\"sentiment\"]\n",
    "\n",
    "# Clear the entire collection\n",
    "res = arima_coll.delete_many({})\n",
    "rest = lstm_coll.delete_many({})\n",
    "\n",
    "# Clear the entire collection\n",
    "# To make it simple to retrieve the data later on we clear the collection\n",
    "# Only one is stored. \n",
    "# If we do not clear the collection to retrieve the data we have to use the timestamp\n",
    "# to retrieve a specific observation\n",
    "result = sentiment_coll.delete_many({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9747c7e-92c9-4a59-a5e6-62d185807dda",
   "metadata": {},
   "source": [
    "# Retrieve Data from MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6b0af-74c3-4eb6-b90e-1adfcf07c8d3",
   "metadata": {},
   "source": [
    "## Company Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e21509-2192-4a7c-91a1-252816054d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT \n",
    "    ticker\n",
    "FROM \n",
    "    Company;\n",
    "\"\"\"\n",
    "\n",
    "company_df = pd.read_sql(query, engine)\n",
    "\n",
    "tickers = company_df['ticker'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a551a52-a936-4533-bdab-b9de3d06c353",
   "metadata": {},
   "source": [
    "# Stocktweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b368030-6349-4e75-8264-2893cada1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocktweet\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT * FROM Stocktweet;\n",
    "\"\"\"\n",
    "\n",
    "# Fetch data into a pandas DataFrame\n",
    "stocktweet_df = pd.read_sql(query, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db0593-a5d9-4448-83a2-f4e31273989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007ee03-93a6-4125-b1f6-3e01799e1374",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b6eaaa-82ff-4738-bd63-55dadf5a56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e68c0-764a-4cf8-832c-9a65cfad6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VADER Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205e1f7b-e27d-4915-8d43-d164b1a9585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSentiment(row):\n",
    "    sentiment_scores = sia.polarity_scores(row['tweet'])\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "stocktweet_df['sentiment_score'] = stocktweet_df.apply(calculateSentiment, axis=1)\n",
    "\n",
    "stocktweet_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b33c12-c538-4afb-ae50-ba4aed8357d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktweet_grouped_df = stocktweet_df.groupby(['date', 'ticker'], as_index=False)['sentiment_score'].mean()\n",
    "stocktweet_grouped_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80998b9-2f7c-4ec4-801d-e657229dcec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktweet_grouped_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885e535d-713a-4dbd-b5c0-972dfbfd885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktweet_grouped_df['date'] = pd.to_datetime(stocktweet_grouped_df['date'], format='%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee8314-7d37-4847-862d-f431f146b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktweet_grouped_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b088e20-5bc7-4869-9fc0-ae0aab5fe40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocktweet_grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ca3d84-a643-4684-b401-48a28ef42b74",
   "metadata": {},
   "source": [
    "## Save Sentiment scores into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a949a-6394-4323-8f88-7b3e3be990c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Sentiment into MongoDB\n",
    "\n",
    "# Convert DataFrame to dictionary\n",
    "stocktweet_grouped_dict = stocktweet_grouped_df.to_dict(orient='records')\n",
    "\n",
    "# Insert all documents into sentiment collection\n",
    "res = sentiment_coll.insert_many(stocktweet_grouped_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6a7b9-ded3-4550-ae1c-1f0207aa33f8",
   "metadata": {},
   "source": [
    "# Retrieve Stockprice and left join with Sentiment for AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9efece-3d31-4013-b478-ade587c0669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve stockprice for a Company from MySQL\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    Stockprice.ticker,\n",
    "    Stockprice.Date,\n",
    "    Stockprice.Open,\n",
    "    Stockprice.High,\n",
    "    Stockprice.Low,\n",
    "    Stockprice.Close,\n",
    "    Stockprice.AdjClose,\n",
    "    Stockprice.Volume\n",
    "    \n",
    "FROM \n",
    "    Stockprice\n",
    "WHERE\n",
    "    Stockprice.ticker = 'AAPL'\n",
    "\"\"\"\n",
    "\n",
    "# Fetch data into a pandas DataFrame\n",
    "stockprice_df = pd.read_sql(query, engine)\n",
    "stockprice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddb2d0-c325-466f-b8ec-62d5adb69fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprice_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73a84c4-93b8-4d60-9eff-8d610c794f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprice_df['Date'] = pd.to_datetime(stockprice_df['Date'], format='%Y-%m-%d')\n",
    "stockprice_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f4f4e-5be3-477e-bd8b-5c4372e1bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprice_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1211483-89b7-4eeb-8f2d-8638875a04fa",
   "metadata": {},
   "source": [
    "# Apache Spark Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4867e39-f332-4b36-a8cd-224840856ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_trunc, avg, min, max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Start a Spark session\n",
    "spark = SparkSession.builder.appName(\"TimeSeriesPreprocessing\").getOrCreate()\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "sdf = spark.createDataFrame(stockprice_df)\n",
    "\n",
    "sdf = sdf.withColumn(\"Date\", col(\"Date\").cast(\"date\"))\n",
    "\n",
    "daily_data = sdf.groupBy(date_trunc(\"day\", col(\"Date\")).alias(\"date\")) \\\n",
    "                .agg(\n",
    "                    avg(\"value\").alias(\"daily_mean\"),\n",
    "                    min(\"value\").alias(\"daily_min\"),\n",
    "                    max(\"value\").alias(\"daily_max\")\n",
    "                )\n",
    "\n",
    "# 3. Fill missing values \n",
    "window_spec = Window.orderBy(\"Date\")\n",
    "daily_data = daily_data.withColumn(\"daily_mean_filled\", \n",
    "                                   col(\"daily_mean\").fillna(lag(\"daily_mean\", 1).over(window_spec)))\n",
    "\n",
    "# 4. Show the preprocessed data\n",
    "daily_data.show()\n",
    "\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43965f-ee36-4636-9f57-a71fea084dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column Date to perform left join\n",
    "stockprice_df.rename(columns={'Date': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b30a2-b9d1-44d3-a123-f1b21f3f2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a left join on both 'ticker' and 'date'\n",
    "stockprice_merged_df = pd.merge(stockprice_df, stocktweet_grouped_df, on=['ticker', 'date'], how='left')\n",
    "stockprice_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a91ee7-370d-4287-b7b2-5500c6888047",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprice_merged_df.fillna(0, inplace=True)\n",
    "stockprice_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53da94f-8cea-4a1d-a471-66c2f57fcfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprice_merged_df = stockprice_merged_df.drop(columns=['ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ef87b-5bb7-4ad6-b8bf-37fd6205ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprice_merged_df.set_index('date', inplace=True)\n",
    "stockprice_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a4556-ab86-4ace-a8ee-88fb5eebb03e",
   "metadata": {},
   "source": [
    "# Descriptive Statistics for AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3db00-8df3-4c4d-9a97-785fe18d31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stockprice_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5087da-70a3-4eaa-9adc-8c991bc6ef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7a75b-9448-4cd6-ad80-4bf10cca866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cff53f-9319-437e-924c-f34a74ac2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "skim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9af389-3b95-4109-830c-9811f6b5cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b223f-cedb-4170-b59e-666792d96083",
   "metadata": {},
   "source": [
    "# Correlation between Close Price and Sentiment Score for AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc748944-a775-4f1b-afa1-2b74c498eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x=\"Close\", y=\"sentiment_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1a8ffc-cdcb-4eca-808a-466a638df886",
   "metadata": {},
   "source": [
    "# ARIMA Model (all companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e1e9db-27d0-446f-b62d-31752aeae6b0",
   "metadata": {},
   "source": [
    "For each ticker, it calculate the forecast at 1d, 3d, and 7d and saves the data into Mongo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57511f5-5276-4339-92a3-9952d9e21613",
   "metadata": {},
   "source": [
    "The saved data will be used by the Dashboard notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87a27f5-5d57-4f23-9eba-3f8556a1a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate ARIMA model with given order\n",
    "def evaluate_arima_model(order):\n",
    "    try:\n",
    "        model = ARIMA(train, order=order)\n",
    "        model_fit = model.fit()\n",
    "        predictions = model_fit.forecast(steps=len(test))\n",
    "        error = mean_squared_error(test, predictions)\n",
    "        return error\n",
    "    except:\n",
    "        return float(\"inf\")\n",
    "\n",
    "for ticker in tickers:\n",
    "    # Retrieve stockprice for a Company from MySQL\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            Stockprice.Date,\n",
    "            Stockprice.Close\n",
    "        FROM \n",
    "            Stockprice\n",
    "        WHERE\n",
    "            Stockprice.ticker = '{}';\n",
    "    \"\"\".format(ticker)\n",
    "\n",
    "    # Fetch data into a pandas DataFrame\n",
    "    stockprice_df = pd.read_sql(query, engine)\n",
    "\n",
    "    # stockprice_df.fillna(0, inplace=True)\n",
    "    stockprice_df.set_index('Date', inplace=True)\n",
    "    print(ticker)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # Hyperparameter Tuning\n",
    "    # ----------------------------------------------------------------------\n",
    "    \n",
    "    # Define the range of p, d, and q values to try\n",
    "    p_values = range(0, 5)\n",
    "    d_values = range(0, 5)\n",
    "    q_values = range(0, 5)\n",
    "\n",
    "    # Generate all combinations of p, d, q\n",
    "    pdq_combinations = list(itertools.product(p_values, d_values, q_values))\n",
    "\n",
    "    # Train-test split\n",
    "    train_size = int(len(stockprice_df) * 0.8)\n",
    "    train, test = stockprice_df[:train_size], stockprice_df[train_size:]\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    best_score, best_order = float(\"inf\"), None\n",
    "\n",
    "    for order in pdq_combinations:\n",
    "        error = evaluate_arima_model(order)\n",
    "        if error < best_score:\n",
    "            best_score, best_order = error, order\n",
    "        print(f\"ARIMA{order} MSE={error:.3f}\")\n",
    "\n",
    "    print(f\"Best ARIMA{best_order} MSE={best_score:.3f}\")\n",
    "\n",
    "    # Fit and forecast using the best order\n",
    "    model = ARIMA(train, order=best_order)\n",
    "    model_fit = model.fit()\n",
    "\n",
    "\n",
    "    # Forecast for 1 day\n",
    "    forecast_1d = model_fit.forecast(steps=1)\n",
    "    \n",
    "    # Forecast for 3 days\n",
    "    forecast_3d = model_fit.forecast(steps=3)\n",
    "\n",
    "    # Forecast for 7 days\n",
    "    forecast_7d = model_fit.forecast(steps=7)\n",
    "\n",
    "    print(\"FORECAST 1D \")\n",
    "    print(forecast_1d.iloc[0])\n",
    "    print(\"FORECASTS 3D \")\n",
    "    print(forecast_3d.iloc[2])\n",
    "    print(\"FORECASTS 7D \")\n",
    "    print(forecast_7d.iloc[6])\n",
    "    \n",
    "    \n",
    "    doc = {\"ticker\": ticker,\n",
    "         \"1D\": forecast_1d.iloc[0],\n",
    "         \"3D\": forecast_3d.iloc[0],\n",
    "         \"7D\": forecast_7d.iloc[0]}\n",
    "\n",
    "    # Insert forecast into MongoDB Collection\n",
    "    result = arima_coll.insert_one(doc)\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    forecast_values = model_fit.forecast(steps=len(test))\n",
    "    mse = mean_squared_error(test, forecast_values)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(stockprice_df.index, stockprice_df['Close'], label='Close price')\n",
    "    plt.plot(test.index, forecast_values, label='Forecasted Close price', color='red')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.title('Close price')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d66aa-e93b-415a-9600-483cf59ba599",
   "metadata": {},
   "source": [
    "# Neural Network Model (for AAPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1e250-5cff-47f3-bdee-a63425dd1508",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockprice_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d1d5c-5cf0-43f4-b0c3-0e59958fcbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stockprice_merged_df\n",
    "\n",
    "data = df['Close'].values\n",
    "data = data.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare training and test datasets\n",
    "X = []\n",
    "y = []\n",
    "time_step = 60  # Use past 60 days to predict the next value\n",
    "\n",
    "for i in range(time_step, len(scaled_data)):\n",
    "    X.append(scaled_data[i-time_step:i, 0])\n",
    "    y.append(scaled_data[i, 0])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dense(units=25))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=50)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61990ef2-909d-400d-a29c-91ee9a528c03",
   "metadata": {},
   "source": [
    "## LSTM Model Evaluation and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31a7dc-ce11-4fe1-ad99-5d6bd3048e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model performance using RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test_actual, predictions))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# Plot the actual vs predicted stock prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_actual, label='Actual Stock Price')\n",
    "plt.plot(predictions, label='Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction - Actual vs Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153099e-5294-4b05-8c16-0c2094d7d69e",
   "metadata": {},
   "source": [
    "# LSTM on all features (all Companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68eec5-4ec3-4b82-aa91-b1a7ad8f6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tickers[:1]:\n",
    "    # Retrieve stockprice for a Company from MySQL\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        Stockprice.ticker,\n",
    "        Stockprice.Date,\n",
    "        Stockprice.Open,\n",
    "        Stockprice.High,\n",
    "        Stockprice.Low,\n",
    "        Stockprice.Close,\n",
    "        Stockprice.AdjClose,\n",
    "        Stockprice.Volume\n",
    "    FROM \n",
    "        Stockprice\n",
    "    WHERE\n",
    "        Stockprice.ticker = '{}'\n",
    "    \"\"\".format(ticker)\n",
    "    print(ticker)\n",
    "\n",
    "    # Fetch data into a pandas DataFrame\n",
    "    stockprice_df = pd.read_sql(query, engine)\n",
    "\n",
    "    stockprice_df['Date'] = pd.to_datetime(stockprice_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Rename column Date to perform left join\n",
    "    stockprice_df.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "    # Performing a left join on both 'ticker' and 'date'\n",
    "    stockprice_merged_df = pd.merge(stockprice_df, stocktweet_grouped_df, on=['ticker', 'date'], how='left')\n",
    "\n",
    "    stockprice_merged_df.fillna(0, inplace=True)\n",
    "\n",
    "    stockprice_merged_df = stockprice_merged_df.drop(columns=['ticker'])\n",
    "\n",
    "    stockprice_merged_df.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Selecting the features \n",
    "    data = stockprice_merged_df[['Open', 'High', 'Low', 'Close', 'AdjClose', 'Volume', 'sentiment_score']]\n",
    "\n",
    "    # Scale the data \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Define a function to create a dataset with time steps and multiple outputs (1-day, 3-days, 7-days)\n",
    "    def create_dataset(data, time_step=60, forecast_days=[1, 3, 7]):\n",
    "        X, y = [], []\n",
    "        for i in range(time_step, len(data) - max(forecast_days)):\n",
    "            X.append(data[i-time_step:i, :])  # Input features from previous 'time_step' days\n",
    "            # Target: Close price for 1-day, 3-days, and 7-days in the future\n",
    "            y.append([data[i + forecast_day, 3] for forecast_day in forecast_days])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # Create the dataset\n",
    "    time_step = 60  # Look back 60 days to predict the next day\n",
    "    forecast_days = [1, 3, 7]  # Predict 1-day, 3-days, and 7-days ahead for Close price\n",
    "    X, y = create_dataset(scaled_data, time_step, forecast_days)\n",
    "\n",
    "    # Split the dataset into training and testing sets (80% training, 20% testing)\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    # Reshape input data for LSTM [samples, time_steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "    # Step 1: Define a function to build the LSTM model for hyperparameter tuning\n",
    "    def build_model(hp):\n",
    "        model = Sequential()\n",
    "\n",
    "        # Hyperparameter for the number of LSTM units\n",
    "        model.add(LSTM(units=hp.Int('units', min_value=50, max_value=200, step=60), \n",
    "                       return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "        # Hyperparameter for the dropout rate to prevent overfitting\n",
    "        model.add(Dense(units=3))  # Output\n",
    "    \n",
    "        # Hyperparameter for optimizer learning rate\n",
    "        model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
    "                      loss='mean_squared_error')\n",
    "    \n",
    "        return model\n",
    "\n",
    "    # Step 2: Use Keras Tuner to find the best hyperparameters\n",
    "    tuner = kt.Hyperband(build_model, \n",
    "                         objective='val_loss', \n",
    "                         max_epochs=10, \n",
    "                         hyperband_iterations=2, \n",
    "                         directory='my_dir', \n",
    "                         project_name='lstm_tuning')\n",
    "\n",
    "    # Step 3: Perform the hyperparameter search\n",
    "    tuner.search(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "    # Step 4: Get the best hyperparameters\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(best_hyperparameters)\n",
    "\n",
    "    # Step 5: Train the model with the best hyperparameters\n",
    "    best_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    # Step 6: Make predictions with the best model\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predicted and actual Close prices to get back to the original scale\n",
    "    predictions_transformed = []\n",
    "    for i in range(predictions.shape[1]):  # For each forecast (1, 3, 7 days)\n",
    "        predictions_transformed.append(\n",
    "            scaler.inverse_transform(np.concatenate((np.zeros((predictions.shape[0], scaled_data.shape[1] - 1)), \n",
    "                                                     predictions[:, i].reshape(-1, 1)), axis=1))[:, 3]\n",
    "        )\n",
    "\n",
    "    # Inverse transform actual data\n",
    "    y_test_transformed = []\n",
    "    for i in range(y_test.shape[1]):  # For each forecast (1, 3, 7 days)\n",
    "        y_test_transformed.append(\n",
    "            scaler.inverse_transform(np.concatenate((np.zeros((y_test.shape[0], scaled_data.shape[1] - 1)), \n",
    "                                                     y_test[:, i].reshape(-1, 1)), axis=1))[:, 3]\n",
    "        )\n",
    "\n",
    "    # Evaluate the model performance using RMSE for 1-day, 3-day, and 7-day predictions\n",
    "    rmse_1day = np.sqrt(mean_squared_error(y_test_transformed[0], predictions_transformed[0]))\n",
    "    rmse_3day = np.sqrt(mean_squared_error(y_test_transformed[1], predictions_transformed[1]))\n",
    "    rmse_7day = np.sqrt(mean_squared_error(y_test_transformed[2], predictions_transformed[2]))\n",
    "\n",
    "    print(f'Root Mean Squared Error (1-day): {rmse_1day}')\n",
    "    print(f'Root Mean Squared Error (3-day): {rmse_3day}')\n",
    "    print(f'Root Mean Squared Error (7-day): {rmse_7day}')\n",
    "\n",
    "    doc = {\"ticker\": ticker,\n",
    "         \"1D\": predictions_transformed[0][0],\n",
    "         \"3D\": predictions_transformed[1][0],\n",
    "         \"7D\": predictions_transformed[2][0]}\n",
    "\n",
    "    print(doc)\n",
    "\n",
    "    print(\"PREDICTIONS TRANSFORMED\")\n",
    "    print(predictions_transformed)\n",
    "    \n",
    "    # Insert forecast into MongoDB Collection\n",
    "    # result = lstm_coll.insert_one(doc)\n",
    "\n",
    "    # Plot the actual vs predicted Close prices for each forecast horizon\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot for 1-day ahead prediction\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(y_test_transformed[0], label='Actual Close Price (1-day ahead)')\n",
    "    plt.plot(predictions_transformed[0], label='Predicted Close Price (1-day ahead)')\n",
    "    plt.title('1-Day Ahead Close Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot for 3-day ahead prediction\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(y_test_transformed[1], label='Actual Close Price (3-days ahead)')\n",
    "    plt.plot(predictions_transformed[1], label='Predicted Close Price (3-days ahead)')\n",
    "    plt.title('3-Days Ahead Close Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot for 7-day ahead prediction\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(y_test_transformed[2], label='Actual Close Price (7-days ahead)')\n",
    "    plt.plot(predictions_transformed[2], label='Predicted Close Price (7-days ahead)')\n",
    "    plt.title('7-Days Ahead Close Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
