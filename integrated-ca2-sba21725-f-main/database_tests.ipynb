{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119a042-8960-4b4a-9862-32bd02e7f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6219c-655a-41dd-ad29-d940f3adbf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Function to run YCSB command and capture output\n",
    "def run_ycsb(database, workload, operation_type):\n",
    "    command = [\n",
    "        \"ycsb\", operation_type, database,\n",
    "        \"-P\", f\"workloads/{workload}\",\n",
    "        \"-p\", \"recordcount=100000\",   \n",
    "        \"-p\", \"operationcount=10000\", \n",
    "    ]\n",
    "    \n",
    "    # Run the YCSB command and capture the output\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "# Function to parse YCSB output into metrics\n",
    "def parse_ycsb_output(ycsb_output):\n",
    "    metrics = {}\n",
    "    for line in ycsb_output.splitlines():\n",
    "        if line.startswith(\"[READ]\") or line.startswith(\"[WRITE]\"):\n",
    "            operation, metric = line.split(\",\")[0], line.split(\",\")[1:]\n",
    "            for m in metric:\n",
    "                key, value = m.strip().split(\"=\")\n",
    "                metrics[f\"{operation}_{key}\"] = float(value)\n",
    "    return metrics\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "\n",
    "# Databases to compare\n",
    "databases = [\"mongodb\", \"jdbc\"]  \n",
    "\n",
    "# Workloads to run\n",
    "workloads = [\"workloada\", \"workloadb\", \"workloadc\"]\n",
    "\n",
    "# Run YCSB tests and collect results\n",
    "for db in databases:\n",
    "    for workload in workloads:\n",
    "        # Load data into the database\n",
    "        print(f\"Loading data for {db} with {workload}...\")\n",
    "        load_output = run_ycsb(db, workload, \"load\")\n",
    "        load_metrics = parse_ycsb_output(load_output)\n",
    "        \n",
    "        # Run the workload\n",
    "        print(f\"Running workload {workload} for {db}...\")\n",
    "        run_output = run_ycsb(db, workload, \"run\")\n",
    "        run_metrics = parse_ycsb_output(run_output)\n",
    "        \n",
    "        # Record results\n",
    "        results.append({\n",
    "            \"Database\": db,\n",
    "            \"Workload\": workload,\n",
    "            \"LoadMetrics\": load_metrics,\n",
    "            \"RunMetrics\": run_metrics\n",
    "        })\n",
    "\n",
    "# Convert results into DataFrame for analysis\n",
    "df_results = pd.json_normalize(results)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ebed4-070a-4b43-b022-dbcada793dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze specific metrics\n",
    "throughput_df = df_results[[\"Database\", \"Workload\", \"RunMetrics.[READ]_Throughput(ops/sec)\", \"RunMetrics.[WRITE]_Throughput(ops/sec)\"]]\n",
    "latency_df = df_results[[\"Database\", \"Workload\", \"RunMetrics.[READ]_AverageLatency(us)\", \"RunMetrics.[WRITE]_AverageLatency(us)\"]]\n",
    "\n",
    "# Display throughput comparison\n",
    "print(\"Throughput Comparison:\")\n",
    "print(throughput_df)\n",
    "\n",
    "# Display latency comparison\n",
    "print(\"\\nLatency Comparison:\")\n",
    "print(latency_df)\n",
    "\n",
    "# Visualization (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Throughput Comparison Plot\n",
    "sns.barplot(data=throughput_df, x=\"Workload\", y=\"RunMetrics.[READ]_Throughput(ops/sec)\", hue=\"Database\")\n",
    "plt.title(\"Read Throughput Comparison\")\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data=throughput_df, x=\"Workload\", y=\"RunMetrics.[WRITE]_Throughput(ops/sec)\", hue=\"Database\")\n",
    "plt.title(\"Write Throughput Comparison\")\n",
    "plt.show()\n",
    "\n",
    "# Latency Comparison Plot\n",
    "sns.barplot(data=latency_df, x=\"Workload\", y=\"RunMetrics.[READ]_AverageLatency(us)\", hue=\"Database\")\n",
    "plt.title(\"Read Latency Comparison\")\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(data=latency_df, x=\"Workload\", y=\"RunMetrics.[WRITE]_AverageLatency(us)\", hue=\"Database\")\n",
    "plt.title(\"Write Latency Comparison\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba302bf-12c8-4c24-ad9e-66818227cc6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
